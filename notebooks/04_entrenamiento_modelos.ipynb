{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f3bb02",
   "metadata": {},
   "source": [
    "# Notebook 04: Entrenamiento y Comparación de Modelos\n",
    "\n",
    "Este notebook entrena y compara múltiples arquitecturas de CNN para reconocimiento de señas ASL:\n",
    "\n",
    "**Modelos a entrenar:**\n",
    "1. SimpleCNN (Baseline)\n",
    "2. ImprovedCNN (Con BatchNormalization)\n",
    "3. DeepCNN (Arquitectura más profunda)\n",
    "4. EfficientNetB0 (Transfer Learning)\n",
    "5. MobileNetV2 (Transfer Learning - Ligero)\n",
    "6. ResNet50 (Transfer Learning - Residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3bf3a3",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar si estamos en Colab\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Ejecutando en Google Colab\")\n",
    "    print(\"Verificando GPU disponible...\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"Ejecutando en entorno local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea179b4",
   "metadata": {},
   "source": [
    "### 1.1. Instalación de Dependencias (Solo en Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Instalando dependencias...\")\n",
    "    !pip install -q kaggle tensorflow opencv-python scikit-learn seaborn plotly\n",
    "    print(\"Dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559e7cf",
   "metadata": {},
   "source": [
    "### 1.2. Configuración de Rutas y Estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    # En Colab, usar /content como raíz\n",
    "    BASE_DIR = Path('/content/asl_project')\n",
    "    BASE_DIR.mkdir(exist_ok=True)\n",
    "    os.chdir(BASE_DIR)\n",
    "else:\n",
    "    # En local, usar directorio del proyecto\n",
    "    BASE_DIR = Path.cwd().parent\n",
    "    os.chdir(BASE_DIR)\n",
    "\n",
    "print(f\"Directorio base: {BASE_DIR}\")\n",
    "\n",
    "# Crear estructura de directorios\n",
    "DIRS = {\n",
    "    'data_raw': BASE_DIR / 'data' / 'raw',\n",
    "    'data_processed': BASE_DIR / 'data' / 'processed',\n",
    "    'models': BASE_DIR / 'models',\n",
    "    'results': BASE_DIR / 'results',\n",
    "    'results_figures': BASE_DIR / 'results' / 'figures',\n",
    "    'results_reports': BASE_DIR / 'results' / 'reports',\n",
    "    'src': BASE_DIR / 'src',\n",
    "    'src_models': BASE_DIR / 'src' / 'models',\n",
    "    'src_utils': BASE_DIR / 'src' / 'utils',\n",
    "}\n",
    "\n",
    "for dir_name, dir_path in DIRS.items():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ {dir_name}: {dir_path}\")\n",
    "\n",
    "# Agregar src al path para imports\n",
    "sys.path.insert(0, str(DIRS['src']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6afd5f",
   "metadata": {},
   "source": [
    "### 1.3. Configuración de Kaggle (si es necesario descargar datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3057b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import json\n",
    "    \n",
    "    # Verificar si ya tenemos datos\n",
    "    if not (DIRS['data_raw'] / 'sign_mnist_train.csv').exists():\n",
    "        print(\"Datos no encontrados. Subir kaggle.json para descargar...\")\n",
    "        print(\"O ejecutar notebook 01_descarga_datos.ipynb primero\")\n",
    "        \n",
    "        # Opcional: subir kaggle.json\n",
    "        # uploaded = files.upload()\n",
    "        # if 'kaggle.json' in uploaded:\n",
    "        #     !mkdir -p ~/.kaggle\n",
    "        #     !mv kaggle.json ~/.kaggle/\n",
    "        #     !chmod 600 ~/.kaggle/kaggle.json\n",
    "    else:\n",
    "        print(\"Datos encontrados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad4ae6",
   "metadata": {},
   "source": [
    "## 2. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6068582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Métricas y utilidades\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Utilidades\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuración de TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Verificar GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\n✓ GPU disponible: {len(gpus)} dispositivo(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"\\n⚠ No se detectó GPU, usando CPU\")\n",
    "\n",
    "# Semillas para reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"\\n✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fcc03",
   "metadata": {},
   "source": [
    "## 3. Cargar Datos Preprocesados\n",
    "\n",
    "Cargamos los datos que fueron preprocesados en el notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar archivos preprocesados\n",
    "processed_files = [\n",
    "    'X_train.npy',\n",
    "    'X_val.npy', \n",
    "    'X_test.npy',\n",
    "    'y_train.npy',\n",
    "    'y_val.npy',\n",
    "    'y_test.npy'\n",
    "]\n",
    "\n",
    "all_files_exist = all(\n",
    "    (DIRS['data_processed'] / f).exists() \n",
    "    for f in processed_files\n",
    ")\n",
    "\n",
    "if not all_files_exist:\n",
    "    print(\"⚠ Archivos preprocesados no encontrados.\")\n",
    "    print(\"Por favor ejecuta el notebook 03_preprocesamiento_avanzado.ipynb primero.\")\n",
    "    print(\"\\nCargando datos raw como alternativa...\")\n",
    "    \n",
    "    # Cargar datos raw\n",
    "    train_df = pd.read_csv(DIRS['data_raw'] / 'sign_mnist_train.csv')\n",
    "    test_df = pd.read_csv(DIRS['data_raw'] / 'sign_mnist_test.csv')\n",
    "    \n",
    "    # Separar features y labels\n",
    "    X_train_raw = train_df.drop('label', axis=1).values\n",
    "    y_train_raw = train_df['label'].values\n",
    "    X_test = test_df.drop('label', axis=1).values\n",
    "    y_test = test_df['label'].values\n",
    "    \n",
    "    # Reshape y normalizar\n",
    "    X_train_raw = X_train_raw.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "    \n",
    "    # Split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_raw, y_train_raw, \n",
    "        test_size=0.15, \n",
    "        random_state=SEED,\n",
    "        stratify=y_train_raw\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Datos cargados desde raw\")\n",
    "else:\n",
    "    # Cargar datos preprocesados\n",
    "    print(\"Cargando datos preprocesados...\")\n",
    "    X_train = np.load(DIRS['data_processed'] / 'X_train.npy')\n",
    "    X_val = np.load(DIRS['data_processed'] / 'X_val.npy')\n",
    "    X_test = np.load(DIRS['data_processed'] / 'X_test.npy')\n",
    "    y_train = np.load(DIRS['data_processed'] / 'y_train.npy')\n",
    "    y_val = np.load(DIRS['data_processed'] / 'y_val.npy')\n",
    "    y_test = np.load(DIRS['data_processed'] / 'y_test.npy')\n",
    "    print(\"✓ Datos preprocesados cargados\")\n",
    "\n",
    "# Información de los datos\n",
    "print(f\"\\nForma de los datos:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "# Número de clases\n",
    "NUM_CLASSES = len(np.unique(y_train))\n",
    "print(f\"\\nNúmero de clases: {NUM_CLASSES}\")\n",
    "\n",
    "# Convertir labels a one-hot encoding\n",
    "y_train_cat = to_categorical(y_train, NUM_CLASSES)\n",
    "y_val_cat = to_categorical(y_val, NUM_CLASSES)\n",
    "y_test_cat = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "print(\"\\n✓ Datos listos para entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40d17b",
   "metadata": {},
   "source": [
    "## 4. Definir Arquitecturas de Modelos\n",
    "\n",
    "Definimos todas las arquitecturas que vamos a entrenar y comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac863aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_cnn(input_shape=(28, 28, 1), num_classes=24):\n",
    "    \"\"\"\n",
    "    CNN Simple - Baseline\n",
    "    \n",
    "    Arquitectura básica con 2 bloques convolucionales.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Bloque 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Capas densas\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='SimpleCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_improved_cnn(input_shape=(28, 28, 1), num_classes=24):\n",
    "    \"\"\"\n",
    "    CNN Mejorada con BatchNormalization\n",
    "    \n",
    "    Incluye normalización por lotes para acelerar convergencia.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Bloque 1\n",
    "        layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(32, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 2\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 3\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Capas densas\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='ImprovedCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_deep_cnn(input_shape=(28, 28, 1), num_classes=24):\n",
    "    \"\"\"\n",
    "    CNN Profunda con Skip Connections\n",
    "    \n",
    "    Arquitectura más profunda inspirada en ResNet.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Bloque inicial\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Bloque residual 1\n",
    "    residual = x\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Bloque residual 2\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    residual = x\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Capas finales\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='DeepCNN')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_transfer_learning_model(base_model_name='EfficientNetB0', \n",
    "                                   input_shape=(28, 28, 1), \n",
    "                                   num_classes=24,\n",
    "                                   trainable=False):\n",
    "    \"\"\"\n",
    "    Modelo usando Transfer Learning\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Nombre del modelo base\n",
    "        input_shape: Forma de entrada\n",
    "        num_classes: Número de clases\n",
    "        trainable: Si las capas del modelo base son entrenables\n",
    "    \"\"\"\n",
    "    # Adaptar entrada para modelos preentrenados\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Convertir grayscale a RGB (los modelos preentrenados esperan 3 canales)\n",
    "    x = layers.Conv2D(3, (1, 1), padding='same')(inputs)\n",
    "    \n",
    "    # Redimensionar a tamaño mínimo requerido\n",
    "    if base_model_name in ['EfficientNetB0', 'MobileNetV2']:\n",
    "        target_size = 32  # Tamaño mínimo aceptable\n",
    "    else:\n",
    "        target_size = 32\n",
    "    \n",
    "    x = layers.Resizing(target_size, target_size)(x)\n",
    "    \n",
    "    # Cargar modelo base\n",
    "    if base_model_name == 'EfficientNetB0':\n",
    "        base_model = tf.keras.applications.EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(target_size, target_size, 3),\n",
    "            pooling='avg'\n",
    "        )\n",
    "    elif base_model_name == 'MobileNetV2':\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(target_size, target_size, 3),\n",
    "            pooling='avg'\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(target_size, target_size, 3),\n",
    "            pooling='avg'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo base no reconocido: {base_model_name}\")\n",
    "    \n",
    "    base_model.trainable = trainable\n",
    "    \n",
    "    # Construir modelo completo\n",
    "    x = base_model(x, training=False)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=f'TL_{base_model_name}')\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"✓ Arquitecturas definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699daa0",
   "metadata": {},
   "source": [
    "## 5. Configuración de Entrenamiento\n",
    "\n",
    "Definimos callbacks, data augmentation y parámetros de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de entrenamiento\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Configuración de entrenamiento:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Épocas: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8263dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_name):\n",
    "    \"\"\"\n",
    "    Crea callbacks para el entrenamiento\n",
    "    \n",
    "    Args:\n",
    "        model_name: Nombre del modelo\n",
    "    \n",
    "    Returns:\n",
    "        Lista de callbacks\n",
    "    \"\"\"\n",
    "    model_path = DIRS['models'] / f\"{model_name}.keras\"\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Guardar mejor modelo\n",
    "        callbacks.ModelCheckpoint(\n",
    "            str(model_path),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Early stopping\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reducir learning rate\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    brightness_range=[0.5, 1.5],  # Importante para webcam\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Generador de validación (sin augmentation)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"\\n✓ Callbacks y data augmentation configurados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f47c1",
   "metadata": {},
   "source": [
    "## 6. Función de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, X_train, y_train, X_val, y_val, \n",
    "                epochs=EPOCHS, batch_size=BATCH_SIZE, use_augmentation=True):\n",
    "    \"\"\"\n",
    "    Entrena un modelo y guarda resultados\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de Keras\n",
    "        model_name: Nombre del modelo\n",
    "        X_train, y_train: Datos de entrenamiento\n",
    "        X_val, y_val: Datos de validación\n",
    "        epochs: Número de épocas\n",
    "        batch_size: Tamaño de batch\n",
    "        use_augmentation: Usar data augmentation\n",
    "    \n",
    "    Returns:\n",
    "        history: Historial de entrenamiento\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Entrenando: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    print(f\"\\nParámetros totales: {model.count_params():,}\")\n",
    "    \n",
    "    # Crear callbacks\n",
    "    callbacks_list = create_callbacks(model_name)\n",
    "    \n",
    "    # Entrenar\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_augmentation:\n",
    "        # Con data augmentation\n",
    "        train_generator = train_datagen.flow(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(X_train) // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        # Sin data augmentation\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✓ Entrenamiento completado en {training_time/60:.2f} minutos\")\n",
    "    \n",
    "    # Guardar historial\n",
    "    history_path = DIRS['results_reports'] / f'{model_name}_history.json'\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'history': str(history.history),\n",
    "            'training_time': training_time,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        }, f)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"✓ Función de entrenamiento definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d96e6",
   "metadata": {},
   "source": [
    "## 7. Entrenar Modelos\n",
    "\n",
    "Entrenamos todos los modelos definidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316fb1e",
   "metadata": {},
   "source": [
    "### 7.1. SimpleCNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d476aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar SimpleCNN\n",
    "model_simple = create_simple_cnn()\n",
    "model_simple.summary()\n",
    "\n",
    "history_simple = train_model(\n",
    "    model_simple, \n",
    "    'SimpleCNN',\n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bcdde",
   "metadata": {},
   "source": [
    "### 7.2. ImprovedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar ImprovedCNN\n",
    "model_improved = create_improved_cnn()\n",
    "model_improved.summary()\n",
    "\n",
    "history_improved = train_model(\n",
    "    model_improved,\n",
    "    'ImprovedCNN',\n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce305432",
   "metadata": {},
   "source": [
    "### 7.3. DeepCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar DeepCNN\n",
    "model_deep = create_deep_cnn()\n",
    "model_deep.summary()\n",
    "\n",
    "history_deep = train_model(\n",
    "    model_deep,\n",
    "    'DeepCNN',\n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed325cad",
   "metadata": {},
   "source": [
    "### 7.4. Transfer Learning - EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe098656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar EfficientNetB0\n",
    "model_efficient = create_transfer_learning_model(\n",
    "    base_model_name='EfficientNetB0',\n",
    "    trainable=False\n",
    ")\n",
    "model_efficient.summary()\n",
    "\n",
    "history_efficient = train_model(\n",
    "    model_efficient,\n",
    "    'TL_EfficientNetB0',\n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    use_augmentation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae764c",
   "metadata": {},
   "source": [
    "### 7.5. Transfer Learning - MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar MobileNetV2\n",
    "model_mobile = create_transfer_learning_model(\n",
    "    base_model_name='MobileNetV2',\n",
    "    trainable=False\n",
    ")\n",
    "model_mobile.summary()\n",
    "\n",
    "history_mobile = train_model(\n",
    "    model_mobile,\n",
    "    'TL_MobileNetV2',\n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    use_augmentation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826f9be",
   "metadata": {},
   "source": [
    "### 7.6. Transfer Learning - ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568960cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar ResNet50\n",
    "model_resnet = create_transfer_learning_model(\n",
    "    base_model_name='ResNet50',\n",
    "    trainable=False\n",
    ")\n",
    "model_resnet.summary()\n",
    "\n",
    "history_resnet = train_model(\n",
    "    model_resnet,\n",
    "    'TL_ResNet50',\n",
    "    X_train, y_train_cat,\n",
    "    X_val, y_val_cat,\n",
    "    use_augmentation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073dc8e",
   "metadata": {},
   "source": [
    "## 8. Evaluación en Conjunto de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de modelos entrenados\n",
    "model_names = [\n",
    "    'SimpleCNN',\n",
    "    'ImprovedCNN', \n",
    "    'DeepCNN',\n",
    "    'TL_EfficientNetB0',\n",
    "    'TL_MobileNetV2',\n",
    "    'TL_ResNet50'\n",
    "]\n",
    "\n",
    "# Evaluar cada modelo\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = DIRS['models'] / f\"{model_name}.keras\"\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"⚠ Modelo no encontrado: {model_name}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluando {model_name}...\")\n",
    "    \n",
    "    # Cargar modelo\n",
    "    model = keras.models.load_model(str(model_path))\n",
    "    \n",
    "    # Evaluar\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Métricas\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred_classes, average='weighted'\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Test Loss': test_loss,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Crear DataFrame de resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTADOS FINALES\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Guardar resultados\n",
    "results_df.to_csv(DIRS['results_reports'] / 'model_comparison.csv', index=False)\n",
    "print(f\"\\n✓ Resultados guardados en {DIRS['results_reports'] / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26460575",
   "metadata": {},
   "source": [
    "## 9. Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de comparación de modelos\n",
    "fig = go.Figure()\n",
    "\n",
    "# Accuracy\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Test Accuracy',\n",
    "    x=results_df['Modelo'],\n",
    "    y=results_df['Test Accuracy'],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "# F1-Score\n",
    "fig.add_trace(go.Bar(\n",
    "    name='F1-Score',\n",
    "    x=results_df['Modelo'],\n",
    "    y=results_df['F1-Score'],\n",
    "    marker_color='lightcoral'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparación de Modelos - Test Set',\n",
    "    xaxis_title='Modelo',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Guardar gráfico\n",
    "fig.write_html(str(DIRS['results_figures'] / 'model_comparison.html'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
